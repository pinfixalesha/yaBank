input {
  kafka {
    bootstrap_servers => "${KAFKA_BOOTSTRAP_SERVERS}"
    topics => ["logs-topic"]
    group_id => "logstash-group"
    consumer_threads => 1
  }
}

filter {
  mutate {
      gsub => [
        "message", "\n", "",
        "message", "  +", " "
      ]
  }

  grok {
    match => {
      "message" => [
        # https://www.javainuse.com/grok
        # Паттерн для основных логов с traceId и spanId
        "^%{TIMESTAMP_ISO8601:timestamp}\s\[%{DATA:thread}\]\s%{LOGLEVEL:level}\s%{DATA:logger}\s\-\s%{DATA:traceId}\:%{DATA:spanId}\s%{DATA:service_name}\s%{GREEDYDATA:message}",

        # Паттерн для логов без traceId/spanId
        "^%{TIMESTAMP_ISO8601:timestamp}\s\[%{DATA:thread}\]\s%{LOGLEVEL:level}\s%{DATA:logger}\s%{GREEDYDATA:message}"
      ]
    }
    tag_on_failure => ["_grokparsefailure"]
  }

  if "_grokparsefailure" in [tags] {
    mutate {
      add_tag => "failed_grok"
    }
  }

  date {
    match => [ "timestamp", "yyyy-MM-dd HH:mm:ss.SSS" ]
    target => "@timestamp"
  }
}

output {
  elasticsearch {
    hosts => ["${ELASTICSEARCH_HOSTS}"]
    index => "logs-%{+YYYY.MM.dd}"
    retry_on_conflict => 3
    action => "index"
  }

  stdout {
    codec => rubydebug {
      metadata => true
    }
  }
}